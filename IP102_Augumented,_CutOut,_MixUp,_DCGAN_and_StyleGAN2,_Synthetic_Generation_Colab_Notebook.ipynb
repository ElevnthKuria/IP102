{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElevnthKuria/IP102/blob/main/IP102_Augumented%2C_CutOut%2C_MixUp%2C_DCGAN_and_StyleGAN2%2C_Synthetic_Generation_Colab_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "S1u6jxfSW_-K",
      "metadata": {
        "id": "S1u6jxfSW_-K"
      },
      "source": [
        "# Importing Libraries and Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "010b6849",
      "metadata": {
        "id": "010b6849"
      },
      "outputs": [],
      "source": [
        "# !pip3 install pandas  matplotlib seaborn scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zvXVCffbVG2s",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvXVCffbVG2s",
        "outputId": "3bd9e19f-cad8-414d-b011-ddda3cf4ba11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OH1e6ccrfY8R",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OH1e6ccrfY8R",
        "outputId": "e16e580f-daf5-492f-cfea-088e6cd45912"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'ip02-dataset' dataset.\n",
            "Downloaded to: /kaggle/input/ip02-dataset\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import kagglehub\n",
        "\n",
        "# Set custom path\n",
        "os.environ[\"KAGGLEHUB_CACHE\"] = \"/content/kagglehub\"\n",
        "\n",
        "path = kagglehub.dataset_download(\"rtlmhjbn/ip02-dataset\")\n",
        "print(\"Downloaded to:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U4C8t1PNouw5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4C8t1PNouw5",
        "outputId": "e19deb1d-1a3e-4a92-d596-fcf8d672e11f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/root/.cache/kagglehub/datasets/rtlmhjbn/ip02-dataset/versions/1/classification': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# ! ls /root/.cache/kagglehub/datasets/rtlmhjbn/ip02-dataset/versions/1/classification"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Root synthetic folder\n",
        "SYNTHETIC_ROOT = Path(\"/content/drive/MyDrive/IP102_Synthetic\")\n",
        "\n",
        "# Define technique names (consistent with the rest of the notebook)\n",
        "TECHNIQUES = [\n",
        "    \"IP102_augmented\",\n",
        "    \"IP102_cutout\",\n",
        "    \"IP102_mixup_cutmix\",\n",
        "    \"DCGAN\",\n",
        "    \"StyleGAN2\",\n",
        "    \"CycleGAN\",\n",
        "    \"cGAN\",\n",
        "    \"VAE\",\n",
        "    \"Diffusion\",\n",
        "    \"DeepSMOTE\"\n",
        "]\n",
        "\n",
        "SELECTED_CLASSES = [7, 8, 9, 11, 59, 69, 72, 94, 98, 101]\n",
        "\n",
        "# Create directory structure\n",
        "for tech in TECHNIQUES:\n",
        "    for cls in SELECTED_CLASSES:\n",
        "        target_dir = SYNTHETIC_ROOT / tech / str(cls)\n",
        "        target_dir.mkdir(parents=True, exist_ok=True)\n",
        "print(\"✅ Folder structure created.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIrH1EZcL8-n",
        "outputId": "874b9a42-acf1-4c5b-89b5-5387c7230b70"
      },
      "id": "SIrH1EZcL8-n",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Folder structure created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b73fcb1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "2b73fcb1",
        "outputId": "af635ccd-7472-45b6-f5bf-6520413df122"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Folder 7 not found.\n",
            "⚠️ Folder 8 not found.\n",
            "⚠️ Folder 9 not found.\n",
            "⚠️ Folder 11 not found.\n",
            "⚠️ Folder 59 not found.\n",
            "⚠️ Folder 69 not found.\n",
            "⚠️ Folder 72 not found.\n",
            "⚠️ Folder 94 not found.\n",
            "⚠️ Folder 98 not found.\n",
            "⚠️ Folder 101 not found.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'num_images'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4042230828.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Save summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_summary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_images\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bar\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Image Count per Class\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Count\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Class ID\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    415\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHashable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_indexing_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'num_images'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define dataset root and selected folders\n",
        "DATASET_PATH = Path(\"/root/.cache/kagglehub/datasets/rtlmhjbn/ip02-dataset/versions/1/classification/train\")\n",
        "SELECTED_FOLDERS = [7, 8, 9, 11, 59, 69, 72, 94, 98, 101]\n",
        "\n",
        "data_summary = []\n",
        "\n",
        "for class_id in SELECTED_FOLDERS:\n",
        "    class_path = DATASET_PATH / str(class_id)\n",
        "    if not class_path.exists():\n",
        "        print(f\"⚠️ Folder {class_id} not found.\")\n",
        "        continue\n",
        "\n",
        "    image_files = list(class_path.glob(\"*.jpg\")) + list(class_path.glob(\"*.png\"))\n",
        "    image_count = len(image_files)\n",
        "\n",
        "    # Optional: sample first image to get resolution\n",
        "    if image_files:\n",
        "        with Image.open(image_files[0]) as img:\n",
        "            width, height = img.size\n",
        "    else:\n",
        "        width, height = (0, 0)\n",
        "\n",
        "    data_summary.append({\n",
        "        \"class_id\": class_id,\n",
        "        \"num_images\": image_count,\n",
        "        \"sample_width\": width,\n",
        "        \"sample_height\": height\n",
        "    })\n",
        "\n",
        "# Save summary\n",
        "df = pd.DataFrame(data_summary)\n",
        "df[\"num_images\"].plot(kind=\"bar\", title=\"Image Count per Class\", figsize=(8, 5))\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xlabel(\"Class ID\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(df)\n",
        "df.to_csv(\"ip102_selected_classes_summary.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NYhSvbUjWk2u",
      "metadata": {
        "id": "NYhSvbUjWk2u"
      },
      "source": [
        "# STEP 1: Classical Augmentation\n",
        "==========================\n",
        "\n",
        "Tools: imgaug, torchvision.transforms, Albumentations\n",
        "\n",
        "Examples: rotations, flips, scaling, brightness, hue, random crops.\n",
        "\n",
        "Implementation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q9im8xavWodO",
      "metadata": {
        "id": "Q9im8xavWodO"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "import os\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Mount Google Drive if dataset is there\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "DATA_PATH = Path(\"/root/.cache/kagglehub/datasets/rtlmhjbn/ip02-dataset/versions/1/classification/train\")  # Example path\n",
        "SAVE_PATH = Path(\"/content/IP102_augmented\")\n",
        "SAVE_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Selected classes\n",
        "SELECTED_CLASSES = [7, 8, 9, 11, 59, 69, 72, 94, 98, 101]\n",
        "\n",
        "# Define augmentations\n",
        "augment = transforms.Compose([\n",
        "    transforms.RandomRotation(25),\n",
        "    transforms.RandomHorizontalFlip(p=0.7),\n",
        "    transforms.RandomVerticalFlip(p=0.3),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.02),\n",
        "    transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0))\n",
        "])\n",
        "\n",
        "# How many augmentations per image\n",
        "AUG_PER_IMAGE = 3\n",
        "\n",
        "for class_id in SELECTED_CLASSES:\n",
        "    src_dir = DATA_PATH / str(class_id)\n",
        "    target_dir = SAVE_PATH / str(class_id)\n",
        "    target_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    image_files = list(src_dir.glob(\"*.jpg\")) + list(src_dir.glob(\"*.png\"))\n",
        "    print(f\"Processing Class {class_id}: {len(image_files)} images\")\n",
        "\n",
        "    for img_path in image_files:\n",
        "        try:\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        for i in range(AUG_PER_IMAGE):\n",
        "            aug_img = augment(img)\n",
        "            save_name = f\"{img_path.stem}_aug{i}.jpg\"\n",
        "            aug_img.save(target_dir / save_name)\n",
        "\n",
        "print(\"✅ Classical Augmentation complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iSncHJGdXiUG",
      "metadata": {
        "id": "iSncHJGdXiUG"
      },
      "outputs": [],
      "source": [
        "# Visualize few augmented examples\n",
        "sample_class = random.choice(SELECTED_CLASSES)\n",
        "sample_folder = SAVE_PATH / str(sample_class)\n",
        "samples = list(sample_folder.glob(\"*.jpg\"))\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i, img_path in enumerate(random.sample(samples, 5)):\n",
        "    img = Image.open(img_path)\n",
        "    plt.subplot(1, 5, i+1)\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "plt.suptitle(f\"Augmented Samples - Class {sample_class}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PyJ-MNL9Wl-2",
      "metadata": {
        "id": "PyJ-MNL9Wl-2"
      },
      "source": [
        "# STEP 2: Random Erasing / Cutout\n",
        "Improves robustness by obscuring random image regions.\n",
        "\n",
        "Tools: torchvision.transforms.RandomErasing, or custom Cutout implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GekLDblTc3gz",
      "metadata": {
        "id": "GekLDblTc3gz"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_PATH = Path(\"/root/.cache/kagglehub/datasets/rtlmhjbn/ip02-dataset/versions/1/classification/train\")\n",
        "SAVE_PATH = Path(\"/content/IP102_cutout\")\n",
        "SAVE_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "SELECTED_CLASSES = [7, 8, 9, 11, 59, 69, 72, 94, 98, 101]\n",
        "\n",
        "# Built-in RandomErasing + Custom Cutout Example\n",
        "cutout_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomErasing(p=0.9, scale=(0.02, 0.2), ratio=(0.3, 3.3), value='random'),\n",
        "])\n",
        "\n",
        "def apply_cutout_and_save(img_path, target_dir, n_aug=2):\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "    for i in range(n_aug):\n",
        "        tensor_img = cutout_transform(image)\n",
        "        cut_img = F.to_pil_image(tensor_img)\n",
        "        cut_img.save(target_dir / f\"{img_path.stem}_cutout{i}.jpg\")\n",
        "\n",
        "for class_id in SELECTED_CLASSES:\n",
        "    src_dir = DATA_PATH / str(class_id)\n",
        "    tgt_dir = SAVE_PATH / str(class_id)\n",
        "    tgt_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    image_files = list(src_dir.glob(\"*.jpg\")) + list(src_dir.glob(\"*.png\"))\n",
        "    print(f\"Applying Cutout to class {class_id} ({len(image_files)} images)\")\n",
        "\n",
        "    for img_path in image_files:\n",
        "        try:\n",
        "            apply_cutout_and_save(img_path, tgt_dir)\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {img_path} -> {e}\")\n",
        "            continue\n",
        "\n",
        "print(\"✅ Random Erasing / Cutout augmentation complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NZ3TfZW0dr9G",
      "metadata": {
        "id": "NZ3TfZW0dr9G"
      },
      "outputs": [],
      "source": [
        "# Preview cutout images\n",
        "import random\n",
        "sample_class = random.choice(SELECTED_CLASSES)\n",
        "sample_folder = SAVE_PATH / str(sample_class)\n",
        "samples = list(sample_folder.glob(\"*.jpg\"))\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i, img_path in enumerate(random.sample(samples, 5)):\n",
        "    img = Image.open(img_path)\n",
        "    plt.subplot(1, 5, i + 1)\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "plt.suptitle(f\"Random Erasing / Cutout Samples - Class {sample_class}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AFlvL5Uydsaq",
      "metadata": {
        "id": "AFlvL5Uydsaq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ijFR9Z3cdm2z",
      "metadata": {
        "id": "ijFR9Z3cdm2z"
      },
      "source": [
        "# STEP 3: MixUp and CutMix Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CmNJjJR1dVWS",
      "metadata": {
        "id": "CmNJjJR1dVWS"
      },
      "outputs": [],
      "source": [
        "# =========================================\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "from PIL import Image\n",
        "import random\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Paths\n",
        "DATA_PATH = Path(\"/root/.cache/kagglehub/datasets/rtlmhjbn/ip02-dataset/versions/1/classification/train\")\n",
        "SAVE_PATH = Path(\"/content/IP102_mixup_cutmix\")\n",
        "SAVE_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "SELECTED_CLASSES = [7, 8, 9, 11, 59, 69, 72, 94, 98, 101]\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# --- Utility to gather filepaths from selected classes ---\n",
        "def get_class_images(cls):\n",
        "    cls_path = DATA_PATH / str(cls)\n",
        "    files = list(cls_path.glob(\"*.jpg\")) + list(cls_path.glob(\"*.png\"))\n",
        "    return files\n",
        "\n",
        "all_images = []\n",
        "for c in SELECTED_CLASSES:\n",
        "    for f in get_class_images(c):\n",
        "        all_images.append((f, c))\n",
        "\n",
        "# --- Base transforms ---\n",
        "base_tf = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "def mixup(img1, img2, alpha=0.3):\n",
        "    \"\"\"Combine two images linearly.\"\"\"\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    return lam * img1 + (1 - lam) * img2, lam\n",
        "\n",
        "def cutmix(img1, img2, alpha=1.0):\n",
        "    \"\"\"Replace rectangular region of img1 with patch from img2.\"\"\"\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    W, H = img1.size()[2], img1.size()[1]\n",
        "    cut_rat = np.sqrt(1. - lam)\n",
        "    cut_w, cut_h = int(W * cut_rat), int(H * cut_rat)\n",
        "\n",
        "    # random center\n",
        "    cx, cy = np.random.randint(W), np.random.randint(H)\n",
        "    x1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    y1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    x2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    y2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "    img1[:, y1:y2, x1:x2] = img2[:, y1:y2, x1:x2]\n",
        "    return img1, lam\n",
        "\n",
        "# --- Generate synthetic samples ---\n",
        "for i in range(80):  # number of synthetic pairs (adjust as needed)\n",
        "    (f1, c1), (f2, c2) = random.sample(all_images, 2)\n",
        "    img1, img2 = base_tf(Image.open(f1).convert(\"RGB\")), base_tf(Image.open(f2).convert(\"RGB\"))\n",
        "\n",
        "    # Randomly choose MixUp or CutMix\n",
        "    if random.random() < 0.5:\n",
        "        aug_img, lam = mixup(img1, img2)\n",
        "        save_dir = SAVE_PATH / \"mixup\"\n",
        "        aug_type = \"MixUp\"\n",
        "    else:\n",
        "        aug_img, lam = cutmix(img1.clone(), img2.clone())\n",
        "        save_dir = SAVE_PATH / \"cutmix\"\n",
        "        aug_type = \"CutMix\"\n",
        "\n",
        "    save_dir.mkdir(parents=True, exist_ok=True)\n",
        "    save_name = f\"{aug_type}_{c1}_{c2}_{str(i).zfill(3)}.jpg\"\n",
        "    save_image(aug_img, save_dir / save_name)\n",
        "\n",
        "print(\"✅ MixUp & CutMix synthetic images generated.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P_ZYb_RkeAoI",
      "metadata": {
        "id": "P_ZYb_RkeAoI"
      },
      "outputs": [],
      "source": [
        "# Display random samples from MixUp & CutMix\n",
        "sample_type = random.choice([\"mixup\", \"cutmix\"])\n",
        "sample_folder = SAVE_PATH / sample_type\n",
        "samples = list(sample_folder.glob(\"*.jpg\"))\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "for i, p in enumerate(random.sample(samples, 5)):\n",
        "    img = Image.open(p)\n",
        "    plt.subplot(1, 5, i+1)\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "plt.suptitle(f\"{sample_type.upper()} Synthetic Samples\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y8VJdjfiea2z",
      "metadata": {
        "id": "y8VJdjfiea2z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "IYeOudkGenZK",
      "metadata": {
        "id": "IYeOudkGenZK"
      },
      "source": [
        "# STEP 4: DCGAN - Deep Convolutional GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "o-aY3gDKfsrG",
      "metadata": {
        "id": "o-aY3gDKfsrG"
      },
      "outputs": [],
      "source": [
        "# =============================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Running on: {device}\")\n",
        "\n",
        "# Path setup\n",
        "DATA_PATH = Path(\"/root/.cache/kagglehub/datasets/rtlmhjbn/ip02-dataset/versions/1/classification/train/7\")  # pick one class for initial training\n",
        "SAVE_PATH = Path(\"/content/DCGAN_output\")\n",
        "SAVE_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Hyperparameters\n",
        "image_size = 64        # reduce memory cost\n",
        "nc = 3                 # RGB\n",
        "nz = 100               # noise vector\n",
        "ngf = 64\n",
        "ndf = 64\n",
        "epochs = 10            # for demo: increase later\n",
        "batch_size = 64\n",
        "lr = 0.0002\n",
        "beta1 = 0.5\n",
        "\n",
        "# Dataset + transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "dataset = torchvision.datasets.ImageFolder(root=DATA_PATH.parent, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "# ---------------------------\n",
        "# Define DCGAN architecture\n",
        "# ---------------------------\n",
        "\n",
        "# Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, nz, ngf, nc):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "# Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, nc, ndf):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(ndf * 4, 1, 4, 2, 1, bias=False),  # --> down to (B,1,H',W')\n",
        "            nn.AdaptiveAvgPool2d(1),                    # --> (B,1,1,1)\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input).view(-1)\n",
        "\n",
        "# Initialize models\n",
        "netG = Generator(nz, ngf, nc).to(device)\n",
        "netD = Discriminator(nc, ndf).to(device)\n",
        "\n",
        "# Loss and optimizers\n",
        "criterion = nn.BCELoss()\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "\n",
        "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
        "\n",
        "# ---------------------------\n",
        "# Training Loop\n",
        "# ---------------------------\n",
        "for epoch in range(epochs):\n",
        "    for i, (real_imgs, _) in enumerate(dataloader):\n",
        "        b_size = real_imgs.size(0)\n",
        "        real_imgs = real_imgs.to(device)\n",
        "        real_label = torch.full((b_size,), 1.0, device=device)\n",
        "        fake_label = torch.full((b_size,), 0.0, device=device)\n",
        "\n",
        "        # (1) Train Discriminator\n",
        "        netD.zero_grad()\n",
        "        output = netD(real_imgs)\n",
        "        lossD_real = criterion(output, real_label)\n",
        "\n",
        "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
        "        fake_imgs = netG(noise)\n",
        "        output = netD(fake_imgs.detach())\n",
        "        lossD_fake = criterion(output, fake_label)\n",
        "        lossD = lossD_real + lossD_fake\n",
        "        lossD.backward()\n",
        "        optimizerD.step()\n",
        "\n",
        "        # (2) Train Generator\n",
        "        netG.zero_grad()\n",
        "        output = netD(fake_imgs)\n",
        "        lossG = criterion(output, real_label)\n",
        "        lossG.backward()\n",
        "        optimizerG.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] | D_loss: {lossD.item():.3f} | G_loss: {lossG.item():.3f}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        fake = netG(fixed_noise).detach().cpu()\n",
        "        save_image(fake, SAVE_PATH / f\"fake_epoch_{epoch+1:03}.png\", normalize=True, nrow=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SKpRfqiwf9li",
      "metadata": {
        "id": "SKpRfqiwf9li"
      },
      "outputs": [],
      "source": [
        "# Display the latest epoch synthetic images\n",
        "latest_img = sorted(SAVE_PATH.glob(\"fake_epoch_*.png\"))[-1]\n",
        "img = Image.open(latest_img)\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(img)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"DCGAN Synthetic Samples\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8IRavS6QhrRs",
      "metadata": {
        "id": "8IRavS6QhrRs"
      },
      "source": [
        "# STEP 5: StyleGAN2‑ADA‑PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D8pkXriCh4Tf",
      "metadata": {
        "id": "D8pkXriCh4Tf"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git\n",
        "%cd stylegan2-ada-pytorch\n",
        "\n",
        "# Install dependencies\n",
        "! pip install ninja opensimplex requests tqdm matplotlib\n",
        "\n",
        "# Optional: connect to Google Drive for dataset and outputs\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q3C8ZZ3CiC4H",
      "metadata": {
        "id": "Q3C8ZZ3CiC4H"
      },
      "outputs": [],
      "source": [
        "# ======================\n",
        "# Preparing the dataset\n",
        "# ======================\n",
        "# /content/IP102_selected/\n",
        "#    ├── class8/\n",
        "#    ├── class9/\n",
        "#    └── class10/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Iux-4k8Diy8U",
      "metadata": {
        "id": "Iux-4k8Diy8U"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Example paths\n",
        "# Assuming: /content/drive/MyDrive/IP102/classes/\n",
        "!mkdir -p /content/IP102_selected/{class8,class9,class10}\n",
        "\n",
        "# Copy from your Drive folders (adjust source)\n",
        "!cp \"/root/.cache/kagglehub/datasets/rtlmhjbn/ip02-dataset/versions/1/classification/train/7\"*.jpg /content/IP102_selected/class8/\n",
        "!cp \"/root/.cache/kagglehub/datasets/rtlmhjbn/ip02-dataset/versions/1/classification/train/8\"*.jpg /content/IP102_selected/class9/\n",
        "!cp \"/root/.cache/kagglehub/datasets/rtlmhjbn/ip02-dataset/versions/1/classification/train/9\"*.jpg /content/IP102_selected/class10/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t5ueTkqeiRFz",
      "metadata": {
        "id": "t5ueTkqeiRFz"
      },
      "outputs": [],
      "source": [
        "!python3 dataset_tool.py --source=/content/IP102_selected \\\n",
        "                       --dest=./datasets/ip102_stylegan \\\n",
        "                       --resolution=256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5eGfmR5ciYSx",
      "metadata": {
        "id": "5eGfmR5ciYSx"
      },
      "outputs": [],
      "source": [
        "# Example: initialize from AFHQ-cat (similar texture variety)\n",
        "!wget https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/afhqcat.pkl -O pretrained.pkl\n",
        "\n",
        "!python train.py \\\n",
        "  --outdir=./training-runs/ip102-stylegan \\\n",
        "  --data=./datasets/ip102_stylegan \\\n",
        "  --gpus=1 \\\n",
        "  --batch=16 \\\n",
        "  --cfg=auto \\\n",
        "  --mirror=1 \\\n",
        "  --resume=pretrained.pkl \\\n",
        "  --gamma=10 \\\n",
        "  --snap=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hABkSZ8yiaY9",
      "metadata": {
        "id": "hABkSZ8yiaY9"
      },
      "outputs": [],
      "source": [
        "# Pick snapshot name\n",
        "SNAPSHOT=\"./training-runs/ip102-stylegan/00000*/network-snapshot-00010.pkl\"\n",
        "\n",
        "!python generate.py \\\n",
        "  --outdir=./generated_samples \\\n",
        "  --trunc=0.7 \\\n",
        "  --seeds=0-49 \\\n",
        "  --network=$SNAPSHOT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dDwp5v0hibYP",
      "metadata": {
        "id": "dDwp5v0hibYP"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "imgs = sorted(glob.glob('./generated_samples/*.png'))[:8]\n",
        "plt.figure(figsize=(12,6))\n",
        "for i, path in enumerate(imgs):\n",
        "    plt.subplot(2,4,i+1)\n",
        "    img = Image.open(path)\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "plt.suptitle(\"StyleGAN2‑ADA Synthetic Insect Samples\", fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LftimWgJtfRF",
      "metadata": {
        "id": "LftimWgJtfRF"
      },
      "source": [
        "# STEP 6: CycleGAN — Cross‑Domain Image Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZDw_PD_7trSJ",
      "metadata": {
        "id": "ZDw_PD_7trSJ"
      },
      "outputs": [],
      "source": [
        "# Clone lightweight CycleGAN repo\n",
        "!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix.git\n",
        "%cd pytorch-CycleGAN-and-pix2pix\n",
        "!pip install dominate visdom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bA-rpkj0tx3T",
      "metadata": {
        "id": "bA-rpkj0tx3T"
      },
      "outputs": [],
      "source": [
        "# datasets/ip102A2B/\n",
        "#     ├── trainA/  --> class 8 images\n",
        "#     ├── trainB/  --> class 9 images\n",
        "#     ├── testA/\n",
        "#     └── testB/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4RAOg59st1z3",
      "metadata": {
        "id": "4RAOg59st1z3"
      },
      "outputs": [],
      "source": [
        "!python train.py \\\n",
        "  --dataroot ./datasets/ip102A2B \\\n",
        "  --name ip102A2B_cyclegan \\\n",
        "  --model cycle_gan \\\n",
        "  --batch_size 2 \\\n",
        "  --num_threads 4 \\\n",
        "  --gpu_ids 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ADiSay2jt5Uy",
      "metadata": {
        "id": "ADiSay2jt5Uy"
      },
      "outputs": [],
      "source": [
        "!python test.py \\\n",
        "  --dataroot ./datasets/ip102A2B \\\n",
        "  --name ip102A2B_cyclegan \\\n",
        "  --model test \\\n",
        "  --num_test 20"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "r_Ys8McTtfpX",
      "metadata": {
        "id": "r_Ys8McTtfpX"
      },
      "source": [
        "# STEP 7: Conditional GAN (cGANs)\n",
        "cGANs are trained with labels so that \\(G(z|y)\\) generates an image specifically belonging to class \\(y\\).\n",
        "\n",
        "Ideal for multiclass datasets (like IP102 7–101)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5M7aiQozIhvt",
      "metadata": {
        "id": "5M7aiQozIhvt"
      },
      "outputs": [],
      "source": [
        "import torch, torch.nn as nn\n",
        "from torchvision.utils import save_image\n",
        "import os, numpy as np\n",
        "\n",
        "num_classes = 10\n",
        "nz, ngf, ndf, nc = 100, 64, 64, 3\n",
        "\n",
        "class G_cGAN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.label_emb = nn.Embedding(num_classes, nz)\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(nz, ngf*8, 4, 1, 0),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf*8, ngf*4, 4, 2, 1),\n",
        "            nn.BatchNorm2d(ngf*4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf*4, ngf*2, 4, 2, 1),\n",
        "            nn.BatchNorm2d(ngf*2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf*2, nc, 4, 2, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    def forward(self, noise, labels):\n",
        "        z = noise + self.label_emb(labels).unsqueeze(2).unsqueeze(3)\n",
        "        return self.main(z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZE_dtSiQIi32",
      "metadata": {
        "id": "ZE_dtSiQIi32"
      },
      "outputs": [],
      "source": [
        "z = torch.randn(batch, nz, 1, 1, device=device)\n",
        "labels = torch.randint(0, num_classes, (batch,), device=device)\n",
        "fake = G(z, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qxMQEh7Xtfsg",
      "metadata": {
        "id": "qxMQEh7Xtfsg"
      },
      "source": [
        "# STEP 8: Variational Autoencoder (VAE)\n",
        "\n",
        "\n",
        "VAEs learn an encoding → latent distribution → decoding cycle.\n",
        "\n",
        "You can sample new latent vectors for smooth synthetic variants."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V90Os8lOA_Ve",
      "metadata": {
        "id": "V90Os8lOA_Ve"
      },
      "outputs": [],
      "source": [
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, latent_dim=64):\n",
        "        super().__init__()\n",
        "        self.enc = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 4, 2, 1), nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        self.fc_mu = nn.Linear(128*56*56, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(128*56*56, latent_dim)\n",
        "        self.fc_dec = nn.Linear(latent_dim, 128*56*56)\n",
        "        self.dec = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 3, 4, 2, 1), nn.Sigmoid()\n",
        "        )\n",
        "    def encode(self, x):\n",
        "        h = self.enc(x)\n",
        "        return self.fc_mu(h), self.fc_logvar(h)\n",
        "    def reparam(self, mu, logvar):\n",
        "        std, eps = torch.exp(0.5*logvar), torch.randn_like(logvar)\n",
        "        return mu + eps*std\n",
        "    def decode(self, z):\n",
        "        h = F.relu(self.fc_dec(z)).view(-1,128,56,56)\n",
        "        return self.dec(h)\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparam(mu, logvar)\n",
        "        return self.decode(z), mu, logvar"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lg-cOqXAtgKb",
      "metadata": {
        "id": "lg-cOqXAtgKb"
      },
      "source": [
        "# STEP 9: Diffusion Models (Denoising Diffusion Probabilistic Models)\n",
        "\n",
        "\n",
        "Diffusion models (e.g., DDPM / Stable Diffusion) gradually denoise random noise → image.\n",
        "\n",
        "They yield state‑of‑the‑art fidelity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XDdPcoPpI6Hh",
      "metadata": {
        "id": "XDdPcoPpI6Hh"
      },
      "outputs": [],
      "source": [
        "!pip install diffusers transformers accelerate safetensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Br0ZSyzGI_N5",
      "metadata": {
        "id": "Br0ZSyzGI_N5"
      },
      "outputs": [],
      "source": [
        "from diffusers import DDPMPipeline\n",
        "\n",
        "model_id = \"google/ddpm-cifar10-32\"  # small pretrained diffusion\n",
        "pipe = DDPMPipeline.from_pretrained(model_id)\n",
        "\n",
        "images = pipe(batch_size=8, num_inference_steps=50, output_type=\"pil\").images\n",
        "for i,img in enumerate(images):\n",
        "    img.save(f\"diffusion_synth_{i}.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g9fxQi_3tg4c",
      "metadata": {
        "id": "g9fxQi_3tg4c"
      },
      "source": [
        "# STEP 10: SMOTE for Images (DeepSMOTE)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fU7WrY-7JHO_",
      "metadata": {
        "id": "fU7WrY-7JHO_"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "import torch, os\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Feature extractor\n",
        "resnet = models.resnet18(pretrained=True)\n",
        "resnet.fc = torch.nn.Identity()\n",
        "resnet.eval()\n",
        "\n",
        "tfm = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "SELECTED_CLASSES = [7, 8, 9, 11, 59, 69, 72, 94, 98, 101]\n",
        "DATA_ROOT = \"/root/.cache/kagglehub/datasets/rtlmhjbn/ip02-dataset/versions/1/classification/train/\"\n",
        "\n",
        "all_embeddings, all_labels = [], []\n",
        "\n",
        "for cls in SELECTED_CLASSES:\n",
        "    cls_path = os.path.join(DATA_ROOT, str(cls))\n",
        "    if not os.path.exists(cls_path):\n",
        "        continue\n",
        "\n",
        "    for fname in os.listdir(cls_path)[:50]:  # limit for speed\n",
        "        try:\n",
        "            img_path = os.path.join(cls_path, fname)\n",
        "            img = tfm(Image.open(img_path).convert(\"RGB\")).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                emb = resnet(img).squeeze().numpy()\n",
        "            all_embeddings.append(emb)\n",
        "            all_labels.append(cls)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "X = np.vstack(all_embeddings)\n",
        "y = np.array(all_labels)\n",
        "\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(sampling_strategy='auto', k_neighbors=3, random_state=42)\n",
        "X_res, y_res = smote.fit_resample(X, y)\n",
        "\n",
        "print(f\"Original: {X.shape}, Resampled: {X_res.shape}\")\n",
        "print(\"Class distribution after SMOTE:\")\n",
        "from collections import Counter\n",
        "print(Counter(y_res))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Hg1YBbRMLKSz",
      "metadata": {
        "id": "Hg1YBbRMLKSz"
      },
      "source": [
        "# Part A: Balancing Dataset to 500–1000 Images per Class\n",
        "Strategy:\n",
        "\n",
        "- Downsample Class 101 from 3444 → 1000\n",
        "\n",
        "- Upsample minority classes using available synthetic techniques\n",
        "\n",
        "- Create a balanced dataset ready for CNN training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CjXm7BlgLYrg",
      "metadata": {
        "id": "CjXm7BlgLYrg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "# === Paths ===\n",
        "DATA_ROOT = Path(\"/root/.cache/kagglehub/datasets/rtlmhjbn/ip02-dataset/versions/1/classification/train/\")\n",
        "SYNTHETIC_ROOT = Path(\"/content/drive/MyDrive/IP102_Synthetic\")\n",
        "BALANCED_ROOT = Path(\"/content/drive/MyDrive/IP102_Balanced_Final\")\n",
        "BALANCED_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "SELECTED_CLASSES = [7, 8, 9, 11, 59, 69, 72, 94, 98, 101]\n",
        "TARGET_MIN = 500\n",
        "TARGET_MAX = 1000\n",
        "\n",
        "# === Current counts ===\n",
        "print(\"Current class distribution:\")\n",
        "class_counts = {}\n",
        "for cls in SELECTED_CLASSES:\n",
        "    cls_path = DATA_ROOT / str(cls)\n",
        "    if cls_path.exists():\n",
        "        imgs = list(cls_path.glob(\"*.jpg\")) + list(cls_path.glob(\"*.png\"))\n",
        "        class_counts[cls] = len(imgs)\n",
        "        print(f\"  Class {cls}: {class_counts[cls]} images\")\n",
        "    else:\n",
        "        class_counts[cls] = 0\n",
        "        print(f\"  Class {cls}: NOT FOUND\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"BALANCING PROCESS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# === Balance each class ===\n",
        "for cls in SELECTED_CLASSES:\n",
        "    current_count = class_counts[cls]\n",
        "    balanced_dir = BALANCED_ROOT / str(cls)\n",
        "    balanced_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    original_imgs = list((DATA_ROOT / str(cls)).glob(\"*.*\"))\n",
        "\n",
        "    # Case 1: Class has more than TARGET_MAX → Downsample\n",
        "    if current_count > TARGET_MAX:\n",
        "        print(f\"\\n📉 Class {cls}: Downsampling {current_count} → {TARGET_MAX}\")\n",
        "        sampled = random.sample(original_imgs, TARGET_MAX)\n",
        "        for img in sampled:\n",
        "            shutil.copy(img, balanced_dir / img.name)\n",
        "\n",
        "    # Case 2: Class is within range → Copy as-is\n",
        "    elif TARGET_MIN <= current_count <= TARGET_MAX:\n",
        "        print(f\"\\n✅ Class {cls}: Already balanced ({current_count} images)\")\n",
        "        for img in original_imgs:\n",
        "            shutil.copy(img, balanced_dir / img.name)\n",
        "\n",
        "    # Case 3: Class below TARGET_MIN → Upsample with synthetics\n",
        "    else:\n",
        "        deficit = TARGET_MIN - current_count\n",
        "        print(f\"\\n📈 Class {cls}: Need {deficit} synthetic images (current: {current_count})\")\n",
        "\n",
        "        # Copy all originals first\n",
        "        for img in original_imgs:\n",
        "            shutil.copy(img, balanced_dir / img.name)\n",
        "\n",
        "        # Gather synthetic images from all available techniques\n",
        "        synthetic_sources = [\n",
        "            SYNTHETIC_ROOT / \"IP102_augmented\" / str(cls),\n",
        "            SYNTHETIC_ROOT / \"IP102_cutout\" / str(cls),\n",
        "            SYNTHETIC_ROOT / \"IP102_mixup_cutmix\" / \"mixup\",\n",
        "            SYNTHETIC_ROOT / \"IP102_mixup_cutmix\" / \"cutmix\",\n",
        "            SYNTHETIC_ROOT / \"DCGAN\" / str(cls),\n",
        "            SYNTHETIC_ROOT / \"VAE\",\n",
        "        ]\n",
        "\n",
        "        available_synthetics = []\n",
        "        for src in synthetic_sources:\n",
        "            if src.exists():\n",
        "                available_synthetics.extend(list(src.glob(\"*.jpg\")) + list(src.glob(\"*.png\")))\n",
        "\n",
        "        if len(available_synthetics) == 0:\n",
        "            print(f\"  ⚠️ Warning: No synthetic images found for class {cls}\")\n",
        "            print(f\"  → You need to generate synthetics first using Steps 1-10\")\n",
        "            continue\n",
        "\n",
        "        # Sample synthetics to fill the gap\n",
        "        needed = min(deficit, len(available_synthetics))\n",
        "        sampled_synthetics = random.sample(available_synthetics, needed)\n",
        "\n",
        "        for i, syn_img in enumerate(sampled_synthetics):\n",
        "            new_name = f\"syn_{cls}_{i:05d}.jpg\"\n",
        "            shutil.copy(syn_img, balanced_dir / new_name)\n",
        "\n",
        "        final_count = len(list(balanced_dir.glob(\"*.*\")))\n",
        "        print(f\"  ✅ Final count for class {cls}: {final_count}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✅ BALANCING COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Balanced dataset saved at: {BALANCED_ROOT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2i-dNjtHLucl",
      "metadata": {
        "id": "2i-dNjtHLucl"
      },
      "source": [
        "### Verify Balance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "llKwYykTLoeY",
      "metadata": {
        "id": "llKwYykTLoeY"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "balanced_counts = {}\n",
        "for cls in SELECTED_CLASSES:\n",
        "    balanced_dir = BALANCED_ROOT / str(cls)\n",
        "    if balanced_dir.exists():\n",
        "        balanced_counts[cls] = len(list(balanced_dir.glob(\"*.*\")))\n",
        "    else:\n",
        "        balanced_counts[cls] = 0\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.bar([str(c) for c in balanced_counts.keys()], balanced_counts.values(), color='teal')\n",
        "plt.axhline(500, color='red', linestyle='--', label='Min Target (500)', linewidth=2)\n",
        "plt.axhline(1000, color='orange', linestyle='--', label='Max Target (1000)', linewidth=2)\n",
        "plt.xlabel(\"Class ID\")\n",
        "plt.ylabel(\"Image Count\")\n",
        "plt.title(\"Balanced IP102 Dataset (Final)\")\n",
        "plt.legend()\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{BALANCED_ROOT.parent}/balanced_distribution.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nBalanced class distribution:\")\n",
        "for cls, cnt in balanced_counts.items():\n",
        "    print(f\"  Class {cls}: {cnt} images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VGW7PKYzL9AT",
      "metadata": {
        "id": "VGW7PKYzL9AT"
      },
      "source": [
        "# Part B: Testing Each Technique with CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mTlktCQdMEkY",
      "metadata": {
        "id": "mTlktCQdMEkY"
      },
      "source": [
        "Training one CNN per technique (10 models total), then compare metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iS1CL4BILS60",
      "metadata": {
        "id": "iS1CL4BILS60"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"🚀 Training on: {device}\")\n",
        "\n",
        "# === Hyperparameters ===\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 15\n",
        "LR = 0.001\n",
        "NUM_CLASSES = len(SELECTED_CLASSES)\n",
        "\n",
        "# === Transforms ===\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# === Define technique-specific datasets ===\n",
        "TECHNIQUE_DATASETS = {\n",
        "    \"Baseline (Original Only)\": DATA_ROOT,\n",
        "    \"Classical Augmentation\": SYNTHETIC_ROOT / \"IP102_augmented\",\n",
        "    \"Random Erasing/Cutout\": SYNTHETIC_ROOT / \"IP102_cutout\",\n",
        "    \"MixUp/CutMix\": SYNTHETIC_ROOT / \"IP102_mixup_cutmix\",\n",
        "    \"DCGAN\": SYNTHETIC_ROOT / \"DCGAN\",\n",
        "    \"Balanced (All Techniques)\": BALANCED_ROOT,\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "# === Training loop for each technique ===\n",
        "for technique_name, data_path in TECHNIQUE_DATASETS.items():\n",
        "    if not data_path.exists():\n",
        "        print(f\"⚠️ Skipping {technique_name} — path not found: {data_path}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"🔬 Training: {technique_name}\")\n",
        "    print(f\"📂 Data path: {data_path}\")\n",
        "    print('='*70)\n",
        "\n",
        "    try:\n",
        "        # Load dataset\n",
        "        dataset = datasets.ImageFolder(root=str(data_path), transform=train_transform)\n",
        "\n",
        "        if len(dataset) == 0:\n",
        "            print(f\"⚠️ No images found\")\n",
        "            continue\n",
        "\n",
        "        # Train/test split\n",
        "        train_size = int(0.8 * len(dataset))\n",
        "        test_size = len(dataset) - train_size\n",
        "        train_set, test_set = torch.utils.data.random_split(\n",
        "            dataset, [train_size, test_size],\n",
        "            generator=torch.Generator().manual_seed(42)  # reproducibility\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "        test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "        # Model (ResNet18 pretrained)\n",
        "        model = models.resnet18(pretrained=True)\n",
        "        model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
        "        model = model.to(device)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "        # Training\n",
        "        for epoch in range(EPOCHS):\n",
        "            model.train()\n",
        "            running_loss = 0.0\n",
        "            for imgs, labels in train_loader:\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(imgs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            avg_loss = running_loss / len(train_loader)\n",
        "            if (epoch + 1) % 5 == 0:\n",
        "                print(f\"  Epoch {epoch+1}/{EPOCHS} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in test_loader:\n",
        "                imgs = imgs.to(device)\n",
        "                preds = model(imgs).argmax(dim=1).cpu().numpy()\n",
        "                all_preds.extend(preds)\n",
        "                all_labels.extend(labels.numpy())\n",
        "\n",
        "        # Metrics\n",
        "        acc = accuracy_score(all_labels, all_preds)\n",
        "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "        print(f\"\\n✅ {technique_name}\")\n",
        "        print(f\"   Accuracy: {acc:.4f}\")\n",
        "        print(f\"   F1 Score: {f1:.4f}\")\n",
        "\n",
        "        results.append({\n",
        "            \"Technique\": technique_name,\n",
        "            \"Accuracy\": acc,\n",
        "            \"F1_Score\": f1,\n",
        "            \"Train_Size\": train_size,\n",
        "            \"Test_Size\": test_size\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error: {e}\")\n",
        "        continue\n",
        "\n",
        "# === Summary ===\n",
        "if len(results) > 0:\n",
        "    df_results = pd.DataFrame(results)\n",
        "    df_results = df_results.sort_values(\"Accuracy\", ascending=False)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"📊 FINAL RESULTS: Technique Performance Comparison\")\n",
        "    print(\"=\"*70)\n",
        "    print(df_results.to_string(index=False))\n",
        "\n",
        "    # Save results\n",
        "    output_path = BALANCED_ROOT.parent / \"technique_comparison_results.csv\"\n",
        "    df_results.to_csv(output_path, index=False)\n",
        "    print(f\"\\n💾 Results saved: {output_path}\")\n",
        "else:\n",
        "    print(\"\\n⚠️ No techniques were successfully trained.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0gIQAfnMMNam",
      "metadata": {
        "id": "0gIQAfnMMNam"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if len(results) > 0:\n",
        "    df_plot = df_results.sort_values(\"Accuracy\", ascending=True)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "    # Accuracy comparison\n",
        "    ax[0].barh(df_plot[\"Technique\"], df_plot[\"Accuracy\"], color='steelblue')\n",
        "    ax[0].set_xlabel(\"Test Accuracy\")\n",
        "    ax[0].set_title(\"CNN Performance by Augmentation Technique\")\n",
        "    ax[0].set_xlim(0, 1)\n",
        "    ax[0].grid(axis='x', alpha=0.3)\n",
        "\n",
        "    # F1 Score comparison\n",
        "    ax[1].barh(df_plot[\"Technique\"], df_plot[\"F1_Score\"], color='coral')\n",
        "    ax[1].set_xlabel(\"F1 Score (Weighted)\")\n",
        "    ax[1].set_title(\"F1 Score by Augmentation Technique\")\n",
        "    ax[1].set_xlim(0, 1)\n",
        "    ax[1].grid(axis='x', alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(BALANCED_ROOT.parent / \"technique_comparison_chart.png\", dpi=150)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dvessfPCMNOw",
      "metadata": {
        "id": "dvessfPCMNOw"
      },
      "source": [
        "# End of Notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AO-LrqxKhyqt",
      "metadata": {
        "id": "AO-LrqxKhyqt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "3.10.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}